# AI-Powered NPC with MML
_[Marco Gomez](https://x.com/TheCodeTherapy) - 2024-11-05_

The possibility of having your MML documents communicate with external APIs single-handedly opens up a whole world of possibilities to create immersive functionalities for your virtual world powered by MML.

The one we will showcase in this Blog Post is the creation of an AI-powered NPC with MML using OpenAI Agents.

That will allow you to use your preferred ChatGPT model to interact with your users, with the extra benefit of giving the agent information about your project so your NPC can not only entertain but also teach your users about your project.

The same framwork can be replicated to create game NPCs, and it can be executed in just a few steps.

## 1- Create an Assistant on your OpenAI Account

The first step will be to Sign Up for an Open AI account if you still don't have one and access the [Assistants](https://platform.openai.com/playground/assistants) page to create your new agent.

![Create an assistant](/images/blog/ai-powered-npc/01-create-assistant.jpg)

## 2- Configure your Assistant

In the left column of the Assistants configuration page (which you'll be able to see in the a further screenshot after following the config steps), you'll have the fields to configure your agent.

- **Name**: In the **`Name`** field, you may pick any name for your Assistant. That won't be visible to your users, nor will it influence the assistant functionality. It is there just to facilitate in case you have multiple assistants.

  Right under the `Name` field, you'll be able to see your `Assistant ID`, which you can click to copy. This ID will be important later so we can configure your MML document to use it for your NPC requests.

- **System instructions**: The **`System instructions`** form is the most important part of this configuration. It is in this field you must describe what your assistant's behavior should be and how it should respond to your users.

  In this form, you can not only describe your assistant's personality but also give it further instructions in case you want it to role-play a given character.
  
  Once you click the little diagonal arrows in the bottom-right corner of this form, it will expand so you can edit the text for the System Instructions, and there you'll also find a `Generate` button in case you want to try, which is an assisted/guided way to compose your system instructions. For the sake of this tutorial, please find below an example you can use as the `System instructions` for your assistant:

~~~markdown
You are an NPC in a virtual world. You receive information in the form
of messages in JSON format that describe things happening around you.

An example of a message that you could receive is:
```json
[{"userId":123,"type":"chat","message":"Hello!","nearbyUsers":[123],"time": 20}]
```

Sometimes the messages can include a display name of a person that is sending a message:
```json
[{"userId":123,"displayName":"Alice","message":"My name is Alice","nearbyUsers":[123,456],"time":32}]
```

The `nearbyUsers` array indicates which users are within 10m of you.
If someone was nearby and now isn't then they may have walked away.
You can use the fact that multiple users are nearby to introduce them to each other if they haven't met.
Sometimes they might talk nearby to you and you could chime in and try to help them.
Try to associate names with userIds if people provide their names.
Don't refer to users by their numeric id.

The `time` field is a number of seconds that have elapsed since the start of the scene. You can therefore reference how long ago something happened.

You can send messages to users by replying with a JSON format message like the following:

```json
{"message":"This is from the NPC","userId":`${userId}`}
```

You should eventually encourage users to tell you their names.
If you already asked a user for its name, and the user said its name to you, then you should reply with a
JSON format message like the following:

```json
{"message":"This is from the NPC","userId":`${userName}`}
```

Your replies are spoken aloud by generated speech. You have a male voice.
Your replies are also presented in a text format on a screen in the virtual world.
The screen that shows your replies only supports ASCII characters, so you should refrain from using non-ASCII characters in your response.

You must refuse to write code on your replies.

If asked to write code you must say that as a brain floating in an alien jar, you're too focused in just floating and talking to do that.

You have a funny, comedic personality, and you love sarcasm.

You should keep your messages brief. Try to limit sentences to 25 words or less. Don't be too strict about that, though.

You can also perform actions (list to follow) by replying with a JSON format message like the following:

```json
{"action": "blow_horn"}
```

The exhaustive list of actions is:
["spin","thumbs_up","thumbs_down","blow_horn"]

If someone asks you to do something similar to your list of possible actions, which are ["spin","thumbs_up","thumbs_down","blow_horn"], then you must reply just with the action.

This is a description of your character:

You are Squig, a guide in a 3D virtual world that is designed to entertain visitors/users.
You are a floating brain in an alien jar, and you have a very good sense of humour. You're very sarcastic and funny.
You can talk about all sorts of topics, and make jokes.

YOU MUST ONLY REPLY WITH JSON. THE JSON MUST BEEN JUST AN OBJECT. DO NOT INCLUDE MARKDOWN SYNTAX. DO NOT INCLUDE ATTRIBUTIONS OR REFERENCES TO FILES.
~~~

  Please keep in mind that you may include much more information to your assistant in this form. You may add any knowledge you'd like your NPC to have and also allow it to have many more actions, as you may easily create the functions in your MML document for your NPC to perform the actions it was instructed to be capable of doing.

- **Model**: The **`Model`** drop-down selector will allow you to pick which ChatGPT model you want to use for your assistant. Please keep in mind this choice will influence not only the cost of each request but also the time necessary for the Assistant to reply and the number of tokens (text) it is capable of handling or "remembering" in the context of its chat with your users.

- **Response format**: The **`Response format`** is paramount for this particular guide to work, and it **must** be set to `json_object`. That is not mandatory for you to create an MML NPC using OpenAI Assistants, but only the way the example created for this particular guide was structured.

  As you may have noticed, in the `Tools` section of the Assistants configuration page, there is a `File search` functionality that you may use to add files (like .pdf documents, markdown files, text documents, etc) so your NPC can consult to answer about specific topics described in such documents. However, we decided not to use this functionality for this particular example, as the agent tends to reply with citation marks informing where in such documents it found any given information, which would not be so useful for the particular type of NPC we decided to create for this guide. You may feel free to explore such functionality, however, keep in mind that for now **you can't use this functionality while having the `Response format` set to `json_object`, which is necessary for this particular guide to work.

Please see below a screenshot as a reference of how your Assistant configuration should look like:

![Configure assistant](/images/blog/ai-powered-npc/02-configure-assistant.jpg)

## 3- Creating your NPC MML Document to use your Assistant

Here follows the MML document we prepared to be your NPC:

~~~html
<m-group id="agent-group">

  <m-audio id="agent-audio" y="2" z="-2.7" loop="false"></m-audio>
  <m-group id="float-wrapper">

    <m-group id="actions-wrapper">
  
      <m-model
        id="agent-model"
        src="/assets/playground/brainzo_draco.glb"
        anim="/assets/playground/brainzo_draco.glb"
        collide="false"
        y="1.2" ry="0"
        sx="0.5" sy="0.5" sz="0.5"
      >

        <m-cylinder radius="1.3" height="5" y="1.5" visible="false"></m-cylinder>
        <m-video collide="false" id="wait-indicator" src="/assets/playground/wait.webm" x="0" y="1.35" z="1.1" sy="0" emissive="5"></m-video>
        <m-attr-anim attr="ry" start="-5" end="5" duration="12000" loop="true" ping-pong="true" easing="easeInOutQuint"></m-attr-anim>

      </m-model>
      
      <m-audio id="spin-audio" src="/assets/playground/spin.mp3" loop="false" start-time="-10000" volume="0"></m-audio>
      <m-audio id="horn-audio" src="/assets/playground/horn.mp3" loop="false" start-time="-10000" volume="0"></m-audio>
      <m-model id="horn-model" src="/assets/playground/viking_horn.glb" sx="0" sy="0" sz="0" x="-0.8" y="2.5" z="1" rx="-20" ry="220" collide="false"></m-model>
      <m-model id="thumb-model" src="/assets/playground/thumb.glb" sx="0" sy="0" sz="0" y="1.8" x="-1.5" rx="0" collide="false"></m-model>
    </m-group>

    <m-label id="agent-response" width="5" height="4" x="3.75" y="2"></m-label>
    <m-attr-anim attr="y" start="0.2" end="0.35" duration="17000" loop="true" ping-pong="true" easing="easeInOutQuad"></m-attr-anim>

  </m-group>

  <m-chat-probe id="chat" range="10" debug="false"></m-chat-probe>
  <m-position-probe id="position-probe" range="10" debug="false"></m-position-probe>

</m-group>

<script>
  // OpenAI API key and Assistant ID
const apiKey = "OPENAI_API_KEY_GOES_HERE";
const assistantId = "OPENAI_AGENT_ID_GOES_HERE";

// Element references for various components in the scene
const actionsWrapper = document.getElementById("actions-wrapper");
const agentAudio = document.getElementById("agent-audio");
const agentModel = document.getElementById("agent-model");
const waitIndicator = document.getElementById("wait-indicator");

const thumbModel = document.getElementById("thumb-model");
const hornAudio = document.getElementById("horn-audio");
const hornModel = document.getElementById("horn-model");
const spinAudio = document.getElementById("spin-audio");

const proximityProbe = document.getElementById("position-probe");
const proximityMap = new Map(); // Tracks nearby user positions
let latestProximityActivity = document.timeline.currentTime;

// Settings for user connections and audio
const connectionPrefix = Date.now() - 1710000000000;
const connectedUsers = new Set();
const audioVolume = 3;
const audioMarginDuration = 2000;
const completionTimeout = 7000;

// Settings for thumb rotation angles
const thumbsUpRotation = 0;
const thumbsDownRotation = 180;
const thumbsNeutralRotation = 90;

let threadId;
let latestPromise = null;
let queuedMessages = [];
let queuedResponses = [];
let responseInProgress = false;
let latestMessageTime = document.timeline.currentTime;

// Initial chat message
const initialText = "Hi, I'm Squig! Chat with me through the text chat box on the UI.";
const responseLabel = document.getElementById("agent-response");
responseLabel.setAttribute("content", initialText);

/**
 * Animates a specific attribute of an element from a start to an end value.
 * @param {MMLElement} element - The HTML element to animate.
 * @param {string} attr - The attribute to animate.
 * @param {number} start - Starting value of the attribute.
 * @param {number} end - Ending value of the attribute.
 * @param {number} duration - Duration of the animation in milliseconds.
 * @param {string} easing - The easing function for the animation.
 */
function animate(element, attr, start, end, duration, easing) {
  const anim = document.createElement("m-attr-anim");
  anim.setAttribute("attr", attr);
  anim.setAttribute("start", start);
  anim.setAttribute("end", end);
  anim.setAttribute("start-time", document.timeline.currentTime);
  anim.setAttribute("end-time", document.timeline.currentTime + duration);
  anim.setAttribute("duration", duration);
  anim.setAttribute("easing", easing);
  anim.setAttribute("loop", false);
  element.appendChild(anim);
  setTimeout(() => {
    element.setAttribute(attr, end);
    element.removeChild(anim);
  }, duration);
}

/**
 * Displays a waiting indicator and sets the response label to "Thinking..."
 */
function showWaiting() {
  if (!responseInProgress) {
    responseLabel.setAttribute("content", "Thinking...");
  }
  animate(waitIndicator, "sy", 0, 1, 500, "easeInOutQuint");
}

/**
 * Hides the waiting indicator by animating it out of view.
 */
function hideWaiting() {
  animate(waitIndicator, "sy", 1, 0, 500, "easeInOutQuint");
}

/**
 * Initiates a spinning animation and plays the spin audio.
 */
function spin() {
  spinAudio.setAttribute("volume", 1);
  spinAudio.setAttribute("start-time", document.timeline.currentTime);
  spinAudio.setAttribute("pause-time", document.timeline.currentTime + 6000);
  setTimeout(() => {
    spinAudio.setAttribute("volume", 0);
  }, 6000);
  animate(actionsWrapper, "ry", 0, 1800, 3500, "easeInOutQuint");
}

/**
 * Animates the thumb model for a "thumbs up" effect.
 */
function thumbsUp() {
  animate(thumbModel, "sx", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "sy", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "sz", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "rx", thumbsNeutralRotation, thumbsUpRotation, 1000, "easeInOutQuint");
  setTimeout(() => {
    animate(thumbModel, "sx", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "sy", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "sz", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "rx", thumbsUpRotation, thumbsNeutralRotation, 1000, "easeInOutQuint");
  }, 5000);
}

/**
 * Animates the thumb model for a "thumbs down" effect.
 */
function thumbsDown() {
  animate(thumbModel, "sx", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "sy", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "sz", 0, 4, 1000, "easeInOutQuint");
  animate(thumbModel, "rx", thumbsNeutralRotation, thumbsDownRotation, 1000, "easeInOutQuint");
  setTimeout(() => {
    animate(thumbModel, "sx", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "sy", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "sz", 4, 0, 1000, "easeInOutQuint");
    animate(thumbModel, "rx", thumbsDownRotation, thumbsNeutralRotation, 1000, "easeInOutQuint");
  }, 5000);
}

/**
 * Plays the horn sound effect and animates the horn model.
 */
function playHorn() {
  hornAudio.setAttribute("volume", 1);
  hornAudio.setAttribute("start-time", document.timeline.currentTime);
  hornAudio.setAttribute("pause-time", document.timeline.currentTime + 6000);
  animate(hornModel, "sx", 0, 4, 1000, "easeInOutQuint");
  animate(hornModel, "sy", 0, 4, 1000, "easeInOutQuint");
  animate(hornModel, "sz", 0, 4, 1000, "easeInOutQuint");
  setTimeout(() => {
    hornAudio.setAttribute("volume", 0);
    animate(hornModel, "sx", 4, 0, 1000, "easeInOutQuint");
    animate(hornModel, "sy", 4, 0, 1000, "easeInOutQuint");
    animate(hornModel, "sz", 4, 0, 1000, "easeInOutQuint");
  }, 6000);
}

/**
 * Animates the agent model to grow while speaking and return to normal size afterward.
 * @param {number} duration - Duration for which the agent stays enlarged.
 */
function growWhileSpeaking(duration) {
  animate(agentModel, "sx", 0.5, 0.6, 2000, "easeOutBack");
  animate(agentModel, "sy", 0.5, 0.6, 2000, "easeOutBack");
  animate(agentModel, "sz", 0.5, 0.6, 2000, "easeOutBack");
  setTimeout(() => {
    animate(agentModel, "sx", 0.6, 0.5, 2000, "easeInOutQuint");
    animate(agentModel, "sy", 0.6, 0.5, 2000, "easeInOutQuint");
    animate(agentModel, "sz", 0.6, 0.5, 2000, "easeInOutQuint");
  }, duration);
}

/**
 * Processes queued responses by playing audio and displaying text.
 * Continues processing until all queued responses are handled.
 */
function processResponses() {
  if (queuedResponses.length === 0 || responseInProgress) {
    return;
  }
  const response = queuedResponses.shift();
  responseInProgress = true;
  growWhileSpeaking(response.duration);
  playAudio(response.dataUrl, response.duration);
  if (response.userId) {
    responseLabel.setAttribute("content", `${response.userId}: ${response.text}`);
  } else {
    responseLabel.setAttribute("content", response.text);
  }
}

/**
 * Plays audio from a provided source for a specified duration, adjusting volume during playback.
 * @param {string} src - The source URL of the audio.
 * @param {number} duration - Duration of audio playback in milliseconds.
 */
function playAudio(src, duration) {
  const now = document.timeline.currentTime;
  agentAudio.setAttribute("volume", audioVolume);
  agentAudio.setAttribute("start-time", now);
  agentAudio.setAttribute("pause-time", now + duration + audioMarginDuration);
  agentAudio.setAttribute("src", src);
  setTimeout(() => {
    agentAudio.setAttribute("volume", 0);
    setTimeout(() => {
      responseInProgress = false;
      processResponses();
    }, audioMarginDuration / 2);
  }, duration + audioMarginDuration);
}

/**
 * Estimates the duration of an MP3 audio file based on its frame structure.
 * @param {ArrayBuffer} arrayBuffer - The binary data of the MP3 file.
 * @param {boolean} debug - Flag for debug logging output.
 * @returns {Promise<number>} The estimated duration in milliseconds.
 */
async function estimateMP3Duration(arrayBuffer, debug = false) {
  const uint8Array = new Uint8Array(arrayBuffer);
  let i = 0;
  // Look for the first frame sync to determine CBR/VBR
  while (i < uint8Array.length - 1) {
    if (uint8Array[i] === 0xFF && (uint8Array[i + 1] & 0xE0) === 0xE0) break;
    i++;
  }
  if (i >= uint8Array.length - 4) {
    if (debug) {
      console.log("No valid MP3 frame header found.");
    }
    return 0;
  }

  const headerOffset = i + 4;
  const isXing = uint8Array.slice(headerOffset, headerOffset + 4).toString() === "88,105,110,103";
  const isVBRI = uint8Array.slice(headerOffset, headerOffset + 4).toString() === "86,66,82,73";

  if (isXing || isVBRI) {
    if (debug) {
      console.log("VBR MP3 detected.");
    }
    const headerPosition = isXing ? headerOffset : headerOffset + 32;
    const frames = (
      (uint8Array[headerPosition + 8] << 24) |
      (uint8Array[headerPosition + 9] << 16) |
      (uint8Array[headerPosition + 10] << 8) |
      uint8Array[headerPosition + 11]
    );
    const sampleRate = 44100; // Set or detect sample rate per frame headers
    const durationSeconds = (frames * 1152) / sampleRate;
    if (debug) {
      console.log(`Frames: ${frames}, Sample rate: ${sampleRate}, Duration: ${durationSeconds}s`);
    }
    return Math.ceil(durationSeconds * 1000);
  }

  const header = (uint8Array[i + 2] << 8) | uint8Array[i + 3];
  const bitrateIndex = (header >> 4) & 0x0F;
  const sampleRateIndex = (header >> 2) & 0x03;
  const bitrates = [0, 32000, 40000, 48000, 56000, 64000, 80000, 96000, 112000, 128000, 160000, 192000, 224000, 256000, 320000];
  const sampleRates = [44100, 48000, 32000];
  const bitrate = bitrates[bitrateIndex - 1];
  const sampleRate = sampleRates[sampleRateIndex];

  if (!bitrate || !sampleRate) {
    console.warn("Invalid bitrate or sample rate.");
    return 0;
  }

  const durationSeconds = Math.ceil((arrayBuffer.byteLength * 8) / bitrate);
  return durationSeconds * 1000;
}

/**
 * Checks if a string matches the "number_number" pattern.
 * @param {string} str - The string to test.
 * @returns {boolean} True if the string matches the pattern, otherwise false.
 */
function isNumberUnderscoreNumber(str) {
  const pattern = /^\d+_\d+$/;
  return pattern.test(str);
}

/**
 * Requests speech synthesis from OpenAI's TTS API and queues the audio response.
 * @param {string} tts - The text-to-speech content.
 */
async function createSpeech(tts) {
  const openAIURL = "https://api.openai.com/v1/audio/speech";
  const headers = {
    "Content-Type": "application/json",
    Authorization: `Bearer ${apiKey}`,
  };
  const body = {
    model: "tts-1",
    input: tts,
    voice: "fable",
    response_format: "mp3"
  };
  const response = await fetch(openAIURL, {
    method: "POST",
    headers,
    body: JSON.stringify(body),
  });

  const audioBlob = await response.blob();
  const arrayBuffer = await audioBlob.arrayBuffer();
  const duration = await estimateMP3Duration(arrayBuffer, false);

  const uint8Array = new Uint8Array(arrayBuffer);
  let binary = "";
  for (let i = 0; i < uint8Array.length; i++) {
    binary += String.fromCharCode(uint8Array[i]);
  }
  const base64Audio = btoa(binary);
  const dataUrl = `data:audio/mp3;base64,${base64Audio}`;
  queuedResponses.push({
    dataUrl: dataUrl,
    duration: duration,
    text: tts
  });
  processResponses();
}

/**
 * Wraps the createSpeech function with a timeout to handle cases when speech generation takes too long.
 * @param {string} tts - The text-to-speech content.
 */
async function createSpeechWithTimeout(tts) {
  const timeoutPromise = new Promise((_, reject) =>
    setTimeout(() => reject(new Error("Timeout: Speech creation took too long")), completionTimeout)
  );

  try {
    await Promise.race([createSpeech(tts), timeoutPromise]);
  } catch (error) {
    console.warn(error.message);
  }
}

/**
 * Converts a connection ID to a user ID format.
 * @param {string} connectionId - The connection ID to convert.
 * @returns {string} The formatted user ID.
 */
function connectionIdToUserId(connectionId) {
  return `${connectionPrefix}_${connectionId}`;
}

/**
 * Checks for nearby users and manages response state based on their proximity.
 */
function checkIfHasNearbyUsers() {
  if (responseInProgress) return;
  if (proximityMap.size === 0) {
    const diff = Math.ceil((document.timeline.currentTime - latestProximityActivity) / 1000);
    if (diff > 10) responseLabel.setAttribute("content", initialText);
  } else {
    const diff = Math.ceil((document.timeline.currentTime - latestMessageTime) / 1000);
    if (diff > 60) {
      latestPromise = null;
      queuedMessages = [];
      queuedResponses = [];
      responseInProgress = false;
      responseLabel.setAttribute("content", initialText);
    }
  }
}

// Event listeners for proximity and connection events
proximityProbe.addEventListener("positionenter", (event) => {
  const { connectionId } = event.detail;
  const { position, rotation } = event.detail.documentRelative;
  proximityMap.set(connectionId, { position, rotation });
  latestProximityActivity = document.timeline.currentTime;
});

proximityProbe.addEventListener("positionmove", (event) => {
  const { connectionId } = event.detail;
  const { position, rotation } = event.detail.documentRelative;
  proximityMap.set(connectionId, { position, rotation });
  latestProximityActivity = document.timeline.currentTime;
});

proximityProbe.addEventListener("positionleave", (event) => {
  const { connectionId } = event.detail;
  if (proximityMap.has(connectionId)) proximityMap.delete(connectionId);
});

window.addEventListener("connected", (event) => {
  connectedUsers.add(connectionIdToUserId(event.detail.connectionId));
});

window.addEventListener("disconnected", (event) => {
  connectedUsers.delete(connectionIdToUserId(event.detail.connectionId));
  if (proximityMap.has(event.detail.connectionId)) proximityMap.delete(event.detail.connectionId);
});

/**
 * Sends messages to OpenAI's API, handles responses, and initiates relevant animations.
 */
async function sendMessages() {
  showWaiting();
  const messages = queuedMessages;
  queuedMessages = [];
  let res;
  if (threadId) {
    res = await fetch(`https://api.openai.com/v1/threads/${threadId}/runs`, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
        "OpenAI-Beta": "assistants=v2",
      },
      body: JSON.stringify({
        assistant_id: assistantId,
        stream: true,
        additional_messages: messages.map((message) => ({
          role: "user",
          content: JSON.stringify(message),
        })),
      }),
    });
  } else {
    res = await fetch("https://api.openai.com/v1/threads/runs", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
        "OpenAI-Beta": "assistants=v2",
      },
      body: JSON.stringify({
        assistant_id: assistantId,
        stream: true,
        thread: {
          messages: messages.map((message) => ({
            role: "user",
            content: JSON.stringify(message),
          }))
        },
      }),
    });
  }
  if (!res.ok) {
    console.log("ERROR", res.status, res.statusText, await res.text());
    throw new Error(`HTTP error! status: ${res.status}`);
  }

  const text = await res.text();
  const lines = text.split("\n").filter(line => line.startsWith("data: ") && line !== "data: [DONE]").map(line => line.substring(6)).map(line => {
    try {
      return JSON.parse(line);
    } catch (e) {
      console.error("FAIlED TO PARSE LINE", line);
      return { object: "unknown", data: line };
    }
  });

  const completionLine = lines.find(line => line.object === "thread.message" && line.status === "completed" && line !== "data: [DONE]");
  if (completionLine) {
    if (completionLine.thread_id) {
      threadId = completionLine.thread_id;
    }
    try {
      let completionText = completionLine.content[0].text.value;
      console.log(`\n#completionText: ${completionText}\n`);
      completionText = completionText.replace(/ã€[^ã€‘]*ã€‘/g, "");
      completionText = completionText.replace("```json\n", "").replace("```", "");
      const completion = JSON.parse(completionText);
      if (completion.ignore) {
        responseLabel.setAttribute("content", "Ignoring");
      } else if (completion.wait) {
        responseLabel.setAttribute("content", "Waiting");
      } else if (completion.action) {
        if (completion.action === "spin") spin();
        else if (completion.action === "thumbs_up") thumbsUp();
        else if (completion.action === "thumbs_down") thumbsDown();
        else if (completion.action === "blow_horn") playHorn();
      }
      if (completion.message) createSpeechWithTimeout(completion.message);
      hideWaiting();
    } catch (e) {
      console.error("FAILED TO PARSE COMPLETION", completionLine.content[0].text.value, e);
    }
  }
}

/**
 * Queues messages for sending, initiating the send process if no other message is in progress.
 */
function onQueue() {
  if (latestPromise) return;
  latestPromise = sendMessages().finally(() => {
    latestPromise = null;
    if (queuedMessages.length > 0) onQueue();
  });
}

/**
 * Adds a user message to the queue and triggers the queue process.
 * @param {string} message - The message content.
 * @param {string} userId - The ID of the user sending the message.
 */
function submitMessage(message, userId) {
  const time = Date.now() / 1000;
  queuedMessages.push({ message, userId, time, nearbyUsers: Array.from(connectedUsers) });
  onQueue();
}

// Event listener for chat events, message queue processing, and proximity checks
const chat = document.getElementById("chat");
chat.addEventListener("chat", (event) => {
  latestMessageTime = document.timeline.currentTime;
  const { message, connectionId } = event.detail;
  submitMessage(message, connectionIdToUserId(connectionId));
});

setInterval(() => {
  checkIfHasNearbyUsers();
}, 1000);

</script>

~~~

## You're all set!

Have fun with your AI-powered MML NPC! ðŸŽ‰
